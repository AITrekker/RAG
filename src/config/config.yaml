# Enterprise RAG Pipeline Configuration
# ===============================

# Application Settings
app:
  name: "Enterprise RAG Pipeline"
  version: "1.0.0"
  environment: "development"  # development, staging, production
  debug: true
  host: "0.0.0.0"
  port: 8000
  workers: 1

# Database Configuration
database:
  # For development (SQLite)
  type: "sqlite"
  sqlite:
    path: "./data/db/enterprise_rag.db"
    echo: false
    check_same_thread: false
  
  # For production (PostgreSQL)
  postgresql:
    host: "localhost"
    port: 5432
    database: "rag_db"
    username: "postgres"
    password: ""  # Set via environment variable
    pool_size: 10
    max_overflow: 20
    pool_timeout: 30
    echo: false

# Vector Store Configuration
vector_store:
  type: "faiss"
  dimension: 768  # For all-MiniLM-L6-v2
  index_type: "IVF_FLAT"
  metric_type: "L2"
  nprobe: 10
  ef_search: 100
  nlist: 1024
  
  # ChromaDB Configuration
  chroma:
    # For Docker deployment
    host: "localhost"  # or "chromadb" for docker-compose
    port: 8001
    
    # For local development
    persist_directory: "./data/vector_store"
    
    # Collection settings
    collection_settings:
      default_ef: 128
      max_connections: 16
      
    # Batch processing
    batch_size: 100
    max_batch_size: 1000
    
  # FAISS Configuration
  faiss:
    index_type: "IndexFlatIP"
    dimension: 1024
    index_path: "./data/indexes/faiss.index"
    
  # Performance settings
  performance:
    enable_cache: true
    cache_size: 1000
    query_timeout: 30
    
  # Embedding settings
  embeddings:
    dimension: 1536  # OpenAI text-embedding-ada-002 or similar
    similarity_metric: "cosine"
    normalize_embeddings: true
    
  # Atomic operations
  operations:
    enable_atomic: true
    rollback_on_failure: true
    operation_timeout: 60
    max_retry_attempts: 3

# Metadata Management Configuration
metadata:
  # Database settings
  database_path: "./data/metadata.db"
  debug_sql: false
  
  # Caching
  enable_cache: true
  cache_ttl: 300  # 5 minutes
  
  # Indexing
  auto_create_indexes: true
  custom_indexes:
    - field: "tenant_id"
      type: "btree"
    - field: "document_type"
      type: "btree"
    - field: "folder_path"
      type: "btree"
    - field: "processing_timestamp"
      type: "btree"
    - field: "content_hash"
      type: "hash"
    - field: "embedding_hash"
      type: "hash"
  
  # Analytics
  enable_analytics: true
  analytics_retention_days: 90
  
  # Relationship tracking
  enable_relationships: true
  relationship_types:
    - "same_folder"
    - "similar_content"
    - "shared_tags"
    - "temporal_proximity"

# ML Models Configuration
models:
  embedding:
    name: "BAAI/bge-large-en-v1.5"
    device: "auto"  # auto, cpu, cuda
    batch_size: 32
    max_length: 512
    normalize_embeddings: true
    cache_dir: "./models/embeddings"
  
  generative:
    name: "microsoft/DialoGPT-medium"
    device: "auto"
    max_length: 1024
    temperature: 0.7
    top_p: 0.9
    do_sample: true
    cache_dir: "./models/generative"
  
  reranker:
    name: "BAAI/bge-reranker-large"
    device: "auto"
    top_k: 10
    cache_dir: "./models/reranker"

# LlamaIndex Configuration
llama_index:
  # Global settings
  global:
    chunk_size: 1000
    chunk_overlap: 200
    context_window: 4000
    enable_logging: true
    log_level: "INFO"
  
  # Tenant-aware settings
  tenant_settings:
    enable_tenant_isolation: true
    shared_models: true  # Share models across tenants for efficiency
    model_sharing_strategy: "memory_efficient"  # memory_efficient, dedicated, hybrid
    tenant_cache_dirs: true  # Create separate cache dirs per tenant
    
  # Model management
  model_management:
    auto_model_loading: true
    lazy_loading: true  # Load models only when needed
    model_unloading: true  # Unload unused models to save memory
    unload_timeout_minutes: 30
    max_concurrent_models: 3
    
  # Resource optimization
  resource_optimization:
    enable_gpu_sharing: true
    batch_processing: true
    async_processing: true
    memory_efficient_loading: true
    quantization: false  # Enable for production with limited GPU memory
    
  # Performance tuning
  performance:
    enable_caching: true
    cache_size_mb: 512
    enable_async_embeddings: true
    parallel_processing: true
    max_parallel_workers: 4

# File Processing Configuration
file_processing:
  master_folder: "./data/master"
  sync_folder: "./data/sync"
  supported_formats:
    - "pdf"
    - "docx"
    - "doc"
    - "pptx"
    - "ppt"
    - "txt"
    - "md"
    - "xlsx"
    - "xls"
    - "csv"
  
  chunking:
    strategy: "recursive"  # recursive, semantic, fixed
    chunk_size: 1000
    chunk_overlap: 200
    separators: ["\n\n", "\n", " ", ""]
  
  metadata_extraction:
    extract_title: true
    extract_author: true
    extract_creation_date: true
    extract_modification_date: true
    extract_file_size: true
    extract_keywords: true

# Sync Configuration
sync:
  interval_hours: 1
  batch_size: 100
  max_file_versions: 3
  report_retention_days: 30
  enable_delta_sync: true
  conflict_resolution: "latest_wins"  # latest_wins, manual, skip

# RAG Pipeline Configuration
rag:
  retrieval:
    top_k: 20
    similarity_threshold: 0.7
    include_metadata: true
    search_strategy: "hybrid"  # semantic, keyword, hybrid
  
  reranking:
    enabled: true
    top_k: 5
    strategy: "cross_encoder"  # cross_encoder, colbert, listwise
  
  generation:
    max_tokens: 512
    temperature: 0.3
    top_p: 0.9
    presence_penalty: 0.1
    frequency_penalty: 0.1
    
  context:
    max_context_length: 4000
    include_source_metadata: true
    citation_style: "academic"  # academic, simple, none

# Multi-Tenancy Configuration
tenancy:
  enabled: false  # Enable for multi-tenant mode
  default_tenant: "tenant1"
  tenant_folders:
    tenant1: "./data/master/tenant1"
    tenant2: "./data/master/tenant2"
  
  isolation:
    database: true
    vector_store: true
    file_system: true
  
  resource_limits:
    max_files_per_tenant: 10000
    max_storage_gb: 50
    max_queries_per_hour: 1000

# Security Configuration
security:
  enable_auth: false  # Enable for production
  jwt:
    secret_key: "${JWT_SECRET_KEY}"
    algorithm: "HS256"
    access_token_expire_minutes: 30
    refresh_token_expire_days: 7
  
  encryption:
    enabled: false
    key: "${ENCRYPTION_KEY}"
    algorithm: "AES-256-GCM"
  
  input_validation:
    max_query_length: 500
    max_file_size_mb: 100
    allowed_file_types: ["pdf", "docx", "txt", "pptx"]
    sanitize_inputs: true

# Resource Management
resources:
  gpu:
    enabled: true
    memory_limit_gb: 10
    memory_fraction: 0.8
  
  cpu:
    max_workers: 4
    max_threads_per_worker: 2
  
  memory:
    max_usage_gb: 8
    cache_size_mb: 512
  
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_size: 10

# Monitoring Configuration
monitoring:
  metrics:
    enabled: true
    prometheus_port: 8000
    export_interval_seconds: 30
  
  tracing:
    enabled: true
    jaeger_endpoint: "http://localhost:14268"
    sample_rate: 0.1
  
  health_checks:
    enabled: true
    interval_seconds: 30
    timeout_seconds: 5

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_path: "logs/rag_pipeline.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5
  
  structured:
    include_timestamp: true
    include_level: true
    include_module: true
    include_function: true
    include_line_number: true
    include_request_id: true

# Development Settings
development:
  auto_reload: true
  debug_queries: true
  profile_performance: false
  mock_external_services: false
  test_data_enabled: true

# Production Settings  
production:
  auto_reload: false
  debug_queries: false
  profile_performance: true
  enable_caching: true
  optimize_models: true
  
  performance:
    connection_pool_size: 20
    max_concurrent_requests: 100
    request_timeout_seconds: 30
    
  scaling:
    auto_scaling: false
    min_replicas: 1
    max_replicas: 5
    target_cpu_utilization: 70

# Embedding deletion configuration
deletion:
  # Queue processing
  max_concurrent_tasks: 3
  batch_size: 100
  enable_verification: true
  
  # Logging and retention
  log_retention_hours: 168  # 7 days
  audit_all_operations: true
  
  # Safety features
  require_confirmation: true
  enable_rollback: true
  backup_before_delete: false  # For production, consider enabling

# Embedding update workflow configuration
update:
  # Transaction management
  max_concurrent_transactions: 2
  enable_verification: true
  auto_cleanup: true
  
  # Processing control
  atomic_operations: true
  rollback_on_failure: true
  
  # Logging and monitoring
  log_retention_hours: 168  # 7 days
  detailed_logging: true
  
  # Performance optimization
  batch_size: 50
  parallel_processing: true
  memory_limit_mb: 1024

# Version tracking configuration
version_tracking:
  # Database configuration
  database_path: "data/versions.db"
  
  # Version management
  max_versions_per_document: 50
  auto_archive_days: 90
  
  # Features
  enable_quality_tracking: true
  enable_auto_comparison: true
  track_performance_metrics: true
  
  # Comparison settings
  comparison_threshold: 0.8
  semantic_comparison: true
  detailed_diff_analysis: true
  
  # Cleanup settings
  cleanup_interval_hours: 24
  max_comparison_cache_size: 1000

# New settings
llm:
  provider: "openai"  # or "anthropic"
  model_name: "gpt-4"
  temperature: 0.7
  max_tokens: 2000
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  stop_sequences: []

processing:
  chunk_size: 500
  chunk_overlap: 50
  max_chunks_per_doc: 100
  embeddings_batch_size: 32
  max_doc_size_mb: 10 