# RAG/LLM Configuration for Easy Tuning
# Copy values to your main .env file to override defaults

# =============================================================================
# LLM MODEL SELECTION
# =============================================================================
# Options: gpt2-medium (345M), gpt2-large (774M), gpt2-xl (1.5B)
#          microsoft/DialoGPT-medium, microsoft/DialoGPT-large
#          distilgpt2 (fast but low quality)
RAG_LLM_MODEL=gpt2-medium

# =============================================================================
# GENERATION QUALITY PARAMETERS (Most Important for Tuning)
# =============================================================================

# Temperature: Lower = more focused/deterministic, Higher = more creative/random
# Range: 0.1 (very focused) to 1.0 (very creative)
# Recommended: 0.2-0.4 for business/factual content
RAG_TEMPERATURE=0.3

# Top-p (Nucleus Sampling): Only sample from top X% probability mass
# Range: 0.1 to 1.0, Recommended: 0.8-0.9
RAG_TOP_P=0.85

# Top-k: Limit vocabulary to top K most likely tokens
# Range: 10-100, Recommended: 30-50 for coherent responses
RAG_TOP_K=40

# Repetition Penalty: Prevent repetitive text
# Range: 1.0 (no penalty) to 2.0 (strong penalty)
# Recommended: 1.1-1.3
RAG_REPETITION_PENALTY=1.3

# =============================================================================
# RESPONSE LENGTH & STRUCTURE
# =============================================================================

# Maximum new tokens to generate (response length)
# Range: 50-500, Recommended: 150-250 for concise answers
RAG_MAX_NEW_TOKENS=200

# Maximum total sequence length
RAG_MAX_LENGTH=512

# Maximum sentences in final response
RAG_MAX_SENTENCES=4

# Minimum sentence length to include
RAG_MIN_SENTENCE_LENGTH=10

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Number of source documents to retrieve and use for context
# Range: 3-10, Recommended: 5-7
RAG_MAX_SOURCES=5

# Confidence threshold for including sources
# Range: 0.1-0.9, Lower = more sources, Higher = only high-confidence
RAG_CONFIDENCE_THRESHOLD=0.3

# Maximum characters in combined context
# Important: Don't exceed model's context window
RAG_MAX_CONTEXT_LENGTH=2000

# Characters to show in source previews
RAG_SOURCE_PREVIEW_LENGTH=200

# =============================================================================
# RESPONSE QUALITY CONTROLS
# =============================================================================

# Remove artifacts from prompt in response
RAG_REMOVE_PROMPT_ARTIFACTS=true

# Ensure sentences end with proper punctuation
RAG_ENSURE_PUNCTUATION=true

# Use sampling vs greedy decoding
RAG_DO_SAMPLE=true

# Stop generation early when complete
RAG_EARLY_STOPPING=true

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================

# Enable model quantization for faster inference (may reduce quality slightly)
RAG_ENABLE_QUANTIZATION=true

# Model cache directory (for faster subsequent loads)
RAG_CACHE_DIR=./cache/transformers

# =============================================================================
# EMBEDDING MODEL (for retrieval)
# =============================================================================

# Embedding model for document similarity
# Options: all-MiniLM-L6-v2 (fast), all-mpnet-base-v2 (better quality)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Device for embeddings: cpu, cuda
EMBEDDING_DEVICE=cpu

# =============================================================================
# PROMPT TEMPLATE SELECTION
# =============================================================================

# Which prompt template to use by default
# Options: default, conversational, technical, executive, qa
# (See src/backend/config/rag_prompts.py for full list)
RAG_DEFAULT_PROMPT_TEMPLATE=default

# =============================================================================
# EXPERIMENTAL/ADVANCED SETTINGS
# =============================================================================

# Alternative model options to try:
# RAG_LLM_MODEL=microsoft/DialoGPT-medium  # Good for conversational responses
# RAG_LLM_MODEL=gpt2-large                 # Higher quality, slower
# RAG_LLM_MODEL=gpt2-xl                    # Best quality, much slower

# Alternative temperature settings to try:
# RAG_TEMPERATURE=0.2   # Very focused, deterministic
# RAG_TEMPERATURE=0.5   # Balanced creativity
# RAG_TEMPERATURE=0.7   # More creative responses

# Alternative top-p settings:
# RAG_TOP_P=0.9   # More diverse vocabulary
# RAG_TOP_P=0.7   # More focused vocabulary

# =============================================================================
# QUICK PRESETS
# =============================================================================

# PRESET: High Quality (slower)
# RAG_LLM_MODEL=gpt2-large
# RAG_TEMPERATURE=0.2
# RAG_MAX_NEW_TOKENS=250
# RAG_MAX_SOURCES=7

# PRESET: Fast Response (lower quality)
# RAG_LLM_MODEL=distilgpt2
# RAG_TEMPERATURE=0.4
# RAG_MAX_NEW_TOKENS=150
# RAG_MAX_SOURCES=3

# PRESET: Creative Responses
# RAG_TEMPERATURE=0.6
# RAG_TOP_P=0.9
# RAG_TOP_K=60

# PRESET: Very Focused/Factual
# RAG_TEMPERATURE=0.1
# RAG_TOP_P=0.7
# RAG_TOP_K=20